"use strict";(self.webpackChunkzeuz_docs=self.webpackChunkzeuz_docs||[]).push([[8749],{91895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"zeuz-platform-20250518","metadata":{"permalink":"/blog/zeuz-platform-20250518","source":"@site/blog/2025-05-18-zeuz-platform-20250518/index.md","title":"ZeuZ Platform 20250518","description":"Release date: May 18, 2025","date":"2025-05-18T00:00:00.000Z","tags":[{"inline":false,"label":"Changelog","permalink":"/blog/tags/changelog","description":"ZeuZ platform changelogs"}],"readingTime":5.59,"hasTruncateMarker":true,"authors":[{"name":"Mohammed Sazid Al Rashid","title":"Technical Lead","url":"https://github.com/sazid","page":{"permalink":"/blog/authors/sazid"},"socials":{"x":"https://x.com/sazidz","linkedin":"https://www.linkedin.com/in/sazidz/","github":"https://github.com/sazid","newsletter":"https://sazid.github.io"},"imageURL":"https://github.com/sazid.png","key":"sazid"}],"frontMatter":{"slug":"zeuz-platform-20250518","title":"ZeuZ Platform 20250518","authors":["sazid"],"tags":["changelog"]},"unlisted":false},"content":"*Release date: May 18, 2025*\\n\\n---\\n\\nWelcome to the May 2025 release of the ZeuZ platform! \ud83d\ude80 We\'re thrilled to introduce several powerful updates that will enhance your testing experience. Key highlights include:\\n\\n## Features\\n\\n- \u2728 Seamless ZeuZ Node connection\\n- \ud83d\udd0d AI-Powered Mobile inspector\\n- \ud83d\udd2e Variable viewer in Debug page\\n- \ud83d\udcca AI-Powered Release report summary\\n- \ud83d\udd04 Pre-requisite test case functionality\\n- \ud83d\udd01 Rerun existing RunID capability\\n- \ud83d\udcc8 Fail analysis reporting\\n\\n\x3c!-- truncate --\x3e\\n\\n## Bug Fixes\\n\\n- Fixed report generation issues when GitHub integration is missing in Testing > Reports > Release Report\\n- Resolved step section slider visibility problems in Testing > Testcase > Create Testcase caused by table insertions in Description or Expected fields\\n- Corrected global attachments scope\\n- Fixed Build-to-Build comparison bugs in Testing > Reports > Execution\\n\\n## Enhancements\\n\\n- Improved assignee user search functionality in Testing > Reports > Fail Analysis\\n- Optimized page load time and performance in Testing > History\\n- Fixed incorrect team/project feature display in Project > Create/Edit > Task, Bug & Requirement pages\\n- Enhanced UI in Testing > Deployments > History & Set page\\n- Restricted Datastore access by team/project for better security\\n- Streamlined GitHub issue linking process\\n- Updated Run Queue feature and UI in Testing > Deployments > History\\n- Added Storybook Integration for UI Components\\n- Enhanced Donut Charts and fixed Total Value in Testing > Deployments > Set/History\\n- Implemented Automatic Default Filter for Fail Analysis Report\\n- Redesigned border color for clarity in Testing > Reports > Fail Analysis\\n- Updated Run History border color to reflect test case status in Testing > Test case edit pages\\n\\n## Seamless ZeuZ Node Connection\\n\\nConnect your ZeuZ Node effortlessly without the hassle of launching a terminal or copying and pasting commands. The process is now completely streamlined!\\n\\n[View the accompanying ZeuZ Node release](https://github.com/AutomationSolutionz/Zeuz_Python_Node/releases/tag/v20.0.0)\\n\\n<video controls loop>\\n  <source src=\\"/blog/zeuz-platform-20250518/Connect ZeuZ Node.mp4\\" type=\\"video/mp4\\" />\\n</video>\\n\\n## AI-Powered Mobile Inspector\\n\\nInspect Android\u2122 applications directly within ZeuZ Server without installing additional tools like Appium Inspector or Android uiautomatorviewer.\\n\\nSay goodbye to hours of setup frustration! Simply connect your mobile device or launch an emulator, run ZeuZ Node, and connect to ZeuZ Server. Everything else is handled automatically. This also solves a persistent challenge:\\n\\n> No more restarting the adb server after each inspection through Appium/uiautomatorviewer.\\n\\n**Features:**\\n\\n1. **Enhanced Screenshot and Element Tree Detection:** Improved accuracy in detecting and displaying UI elements\\n2. **Dedicated Attribute List Panel:** Right-side panel for comprehensive element attribute inspection\\n3. **Powerful Search Functionality:** Use **Ctrl+F** to search by CSS, XPath, or value\\n4. **Dual DOM Support:** Both ADB and Appium now provide DOM access, eliminating inspection barriers when the Appium server is active\\n\\n![Mobile inspector](mobile-inspector.png)\\n\\n## Variable Viewer in Debug Page\\n\\nWhen debugging test cases, users often need to inspect current ZeuZ node variables to build further actions. Our new hierarchical tree view of variables provides a clean, intuitive interface for navigating complex data structures. Variables are displayed in three distinct formats:\\n\\n1. **JSON Object Variables:** Standard JSON-compatible variables with reasonably sized values\\n2. **JSON Schema Variables:** Large JSON variables truncated for readability and presented as schemas\\n3. **Non-JSON Variables:** Non-JSON objects (e.g., Python modules, WebDriver instances, WebDriver elements) with up to 200 properties displayed\\n4. **Oversized Values:** Variables with excessively large values are omitted for performance optimization\\n\\n![Debug variable viewer](debug-variables.png)\\n\\n## AI-Powered Release Report Summary\\n\\nOur intelligent report summary component analyzes release data and presents key insights through a modern UI. This feature enables stakeholders to quickly assess release impact, quality, and notable changes without wading through lengthy detailed reports.\\n\\n![AI-Powered release report summary](ai-powered-release-report-summary.png)\\n\\n## Pre-requisite Test Case\\n\\nThis powerful new feature allows one test case to specify another as its pre-requisite. When a test case is deployed, its pre-requisite test case executes first.\\n\\nWhen multiple test cases share a common pre-requisite and are deployed together, the pre-requisite executes only once throughout the entire RunID (run session), improving efficiency.\\n\\nThe system intelligently prevents cyclic dependencies during both link time and deployment time.\\n\\n**Background:**\\n\\n- Testers typically need pre-test setup like data preparation or environment configuration. Previously, users relied on set re-ordering to ensure pre-setup tests ran first.\\n- Some test sets contain multiple test case groups with their own pre-requisites.\\n- Test cases with pre-requisites may appear in multiple sets.\\n\\nThese factors previously made it challenging to debug or run test cases with pre-requisite steps. Our solution streamlines this process completely.\\n\\n**Pre-requisite search in test case details tab:**  \\n![pre-requisite search in test case details tab](pre-requisite-search-in-tc-detail.png)\\n\\n**Pre-requisite, once selected:**  \\n![Pre-requisite, once selected](pre-requisite-once-selected.png)\\n\\n**Rerun and pre-requisite selection from Testing > Run tests page:**  \\n![Rerun and pre-requisite selection from Testing > Run tests page](rerun-and-pre-requisite-in-run-test.png)\\n\\n## Rerun an Existing RunID\\n\\nUsers can now re-run an existing RunID or selected test cases within a RunID, addressing common operational challenges:\\n\\n1. When a test case fails due to a temporary issue, there\'s no need to rerun an entire test set of 1,000+ cases just to achieve a green RunID status.\\n2. If a node terminates prematurely before sending reports to the server (e.g., due to memory constraints), leaving test cases marked as \\"submitted\\" in the RunID, you can now easily rerun just those affected cases.\\n\\n**Search RunIDs for rerun (Testing > Run tests > Advanced settings):**  \\n![Search runids for rerun](search-runids-for-rerun.png)\\n\\n## Fail Analysis Report\\n\\nWhen multiple test cases fail, identifying root causes and avoiding duplicate bug reports can be challenging. Our Fail Analysis Report solves this by analyzing failed test steps for common failure patterns and grouping them intelligently.\\n\\nWhen one test case in a group is fixed, all related test cases should automatically resolve as long as the steps are global, dramatically improving efficiency.\\n\\n### How It Works\\n\\nAccess the report through **Testing > Reports > Fail Analysis** and create a filter for your analysis.\\n\\n![Fail analysis report: configure filter](fail-analysis-report-filter.gif)\\n\\n- Click on Filter to select Milestone, Version, or GitHub parameters\\n- In this example, we compare Milestone 2.9 with 2.8\\n- The system generates a comprehensive Fail Test Case Report\\n\\n![Description of card view](fail-analysis-card-view-description.png)\\n\\n**Card View Elements:**\\n\\n1. Test case title\\n2. Failed test case step\\n3. Failure reason\\n4. Assignee set\\n5. Assignee feature\\n6. Assignee milestone\\n7. Assignee version\\n8. Comment option (anyone can comment and reply)\\n9. Assignee priority\\n10. Assignee label\\n11. Assignee folder\\n\\n![Fail analysis: card list view](fail-analysis-card-list.png)\\n\\nYou can also view reports categorized by failure type.\\n\\n![Fail analysis: group by different category](fail-analysis-group-by.png)\\n\\n- Track pass/fail statuses at the top of the report\\n- Group reports by:\\n  1. Failed test case\\n  2. Set\\n  3. Assignee\\n- Switch between the Fail tab and the GitHub tab to find linked test cases with GitHub issue filters"}]}}')}}]);